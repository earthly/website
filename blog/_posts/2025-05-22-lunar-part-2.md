---
title: "When the Slack Channel Gets Archived, but the Service Keeps Running"
categories:
  - news
toc: true
author: Vlad
topic: earthly
funnel: 3
topcta: false
excerpt: |
    Last year, our team spent a lot of time interviewing fellow Platform, DevOps, DevEx, CI/CD, and SRE engineers, as well as engineering leaders, in order to better understand their day-to-day challenges. We began this effort to see how Earthfiles, one of our products, could serve engineering teams at scale. But as we spoke to more and more people, we realized that platform engineering as an industry is on a collision course with something far more painful and visceral than just build speed.
last_modified_at: 2025-04-16
---

TL;DR: We interviewed 100+ companies to understand how they manage engineering governance in the face of dev setup entropy. This post breaks down six patterns we saw — from CI templates to scorecards to DIY platforms — and shows where each one succeeds, fails, or quietly collapses under real-world pressure.

> It still runs.
>
> The service, buried deep in the infrastructure, still receives traffic. Still logs errors. Still serves a feature that someone, somewhere, might rely on. No one remembers exactly what it does. The README is out of date. The Slack channel was archived two layoffs ago.
>
> Once, it had a team. A mission. Engineers sat in a pod, under warm fluorescent lights, planning features and joking over bad coffee. They gave the service a clever name. They wrote dashboards. They argued over endpoint naming conventions.
>
> And then time passed.
>
> Reorgs swept through. Teams were renamed, merged, dissolved. People left. Ownership blurred. Documentation drifted.
>
> Today, it's still deployed — but untended. Not quite dead. Not quite alive. Ask the SRE team, and you'll hear a sigh: "We think it's critical, but nobody owns it anymore."

We heard dozens of stories like this. Different details. Same themes.

A service running in limbo.
A checklist ticked, but never read.
A vulnerability found, but never fixed.

This is how engineering chaos creeps in. Not through malice — but through entropy. Through drift. Through the thousand tiny decisions that feel right in isolation, but accumulate into a system no one fully understands or controls.

[In a previous article](https://earthly.dev/blog/lunar-launch/), we explored the root causes: microservices, the explosion of tooling, the limits of standardization, and the widening gap between policy and reality.

This time, we're going deeper into the coping mechanisms — what companies are actually doing to survive. These aren't best practices. They're stress responses. Workarounds. Cultural antibodies. Sometimes admirable. Sometimes alarming.

If you've ever asked yourself, "How are we still standing?" — this article is for you. In the sections below, we've captured the six most common patterns we saw — not as endorsements, but as field notes. What's working. What's breaking. And what it really looks like to fight chaos at scale.
